"task_specific_params": {
    "conversational": {
      "max_length": 1000
    }
  },


The maximum number of tokens the model is allowed to generate.
This includes both - 'input + output' length.
In conversational setting this is 'history + input + output' length.


# -----------------------------------------------------

"n_ctx": 1024,

This is the 'context_length' of the model in terms of number of tokens. However, this is not just the input length but BOTH - 
number of tokens in the input PLUS number of tokens left for output.

So, if we need say n_ctx is 1024 and we feed input of length 900; there is only 124 token length left for the possible output in this forward pass.
If input is of length 1024; then no space left for output.

So, 'max_length' < 'n_ctx' if we expect any output from the model.


# -------------------------------------------------------

"n_embd": 1024,

This refers to the dimensionality of the embedding vectors
It's often called the "hidden size" or "embedding dimension" in transformer models.
This is typically kept the same as the output size of the transformer blocks.
It's a fundamental dimension in the model architecture - and is not related to the input length.

# -------------------------------------------------------

tokenizer_config.json

"model_max_length": 1024,

The maximum length at which the tokenizer will truncate the input.
This typically aligns with the model's maximum context length (n_ctx) to ensure the tokenized input fits the model's capabilities.


# -------------------------------------------------------
tokenizer_config.json

Padding behavior:

By default, even with padding=True, the tokenizer will NOT automatically pad to the full model_max_length (1024 in this case).
The tokenizer will pad to the length of the longest sequence in the batch, not necessarily to model_max_length.
This is more efficient.

Specifying max_length: To pad to a specific length, you need to explicitly specify the max_length parameter when calling the tokenizer.
	tokenizer(texts, padding=True, truncation=True, max_length=1024, return_tensors="pt")
Padding to exact length can be inefficient based on the input.

# ----------------------------------------------------------

config.json

"architectures": [
    "GPT2LMHeadModel"
  ],


This is the pre-trained model used by Dialo-GPT-medium model for further finetuning.
GPT-2 Language Modeling Head Model.
We are NOT slicing off the head of the pre-trained model.

GPT-2 is pretrained on the next token prediction classification task and we retain this flow.

This is why output size from the mode.predict() is (batch_size, seq_len, vocab_size) 

# ----------------------------------------------------------




# check if we can truncate the history
            if curr_hist_len > int(curr_tot_ip_len * cfg['max_hist_input_prop']):
                slice_start_idx = curr_hist_len - max_hist_ip_len
                bot_input_ids = bot_input_ids[slice_start_idx:]
                bot_attention_mask = bot_attention_mask[slice_start_idx:]
            
            # check against, if we need to truncate the current input
            curr_hist_len = bot_input_ids.shape[-1]
            curr_tot_ip_len = curr_ip_len + curr_hist_len
            if curr_tot_ip_len > int(cfg['max_len'] * cfg['max_tot_input_prop']):
                
        





